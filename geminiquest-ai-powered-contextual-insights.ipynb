{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":97258,"sourceType":"competition"}],"dockerImageVersionId":31012,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/dhivyamarimuthu/geminiquest-ai-powered-contextual-insights?scriptVersionId=234627362\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"#                       **GeminiQuest: AI-Powered Contextual Insights**","metadata":{}},{"cell_type":"markdown","source":"## **Introduction**\n\nThis project aims to showcase the power of **Google Generative AI (Gemini)** in creating an intelligent, context-aware chatbot capable of answering queries from a specific corpus of text. By leveraging **semantic search** and **text embeddings**, the chatbot is designed to provide accurate, relevant, and natural language responses to user queries, transforming the way information is retrieved and processed.\n\nThe underlying use case involves processing a large dataset (such as *Alice’s Adventures in Wonderland*) and building a knowledge base that can be dynamically queried. Through **Google’s embedding model**, the text is transformed into high-dimensional vectors optimized for document retrieval tasks. By utilizing **Google Gemini's** capabilities, this system can understand, interpret, and generate human-like responses to complex queries, offering a highly interactive and intuitive AI solution.\n\nThis project highlights the integration of AI technologies to enable efficient, scalable, and intelligent information retrieval. With **Generative AI capabilities** at the core, the project demonstrates a seamless approach to building smarter systems for contextual information processing, which has wide applications in industries ranging from **customer support** to **education** and **content management**.\n","metadata":{}},{"cell_type":"markdown","source":"### **Gen AI Capabilities Demonstrated in the Capstone Project**\n\nThis project leverages several advanced capabilities of **Google Generative AI** (Gemini), ensuring that the system is not only highly efficient but also capable of producing intelligent and contextually aware responses. Below are the three key capabilities demonstrated in the project:\n\n1. **Embeddings**:  \n   **Text Embeddings** are generated using Google's **Generative AI** to convert the content (such as *Alice's Adventures in Wonderland*) into high-dimensional vectors. This allows for semantic search, enabling the system to retrieve relevant information based on the meaning rather than just keywords. The embeddings are crucial in ensuring that the responses are contextually appropriate and aligned with the user's query.\n\n2. **Retrieval Augmented Generation (RAG)**:  \n   The system integrates **retrieval augmented generation** by combining **document retrieval** with generative capabilities. The relevant context from the document is retrieved using **semantic search** and then augmented with the generative abilities of Google Gemini to create more accurate and context-aware responses. This hybrid approach enhances the system's ability to provide more reliable and relevant information.\n\n3. **Document Understanding**:  \n   The project incorporates **document understanding** by processing large text corpora and enabling the chatbot to answer questions based on the content. The system uses the **Gemini model** to comprehend and analyze documents, extracting key information and transforming it into usable knowledge that can be queried by users. This understanding is essential for providing coherent, accurate answers to complex questions.\n\n4. **Gen AI Evaluation**:   \nGen AI evaluation techniques are employed to assess the effectiveness of the generated responses in meeting predefined quality criteria such as relevance, accuracy, coherence, fluency, and conciseness. The evaluation process includes pointwise analysis of individual question-answer pairs to ensure that the responses not only align with user expectations but also provide meaningful and contextually relevant information. Specific evaluation methods include:\n\n- **Relevance**: Ensuring that the response appropriately addresses the query.\n- **Accuracy**: Verifying that the information provided is correct.\n- **Coherence**: Checking if the answer is logically structured and easy to understand.\n- **Fluency**: Assessing whether the language is natural and error-free.\n- **Conciseness**: Ensuring the response is clear and to the point.\n- **Quality**: Determining whether the answer provides valuable insights based on the query.\n\n\nBy combining these capabilities, the project demonstrates an intelligent approach to content processing, retrieval, and generation, paving the way for more sophisticated AI applications in real-world scenarios.\n","metadata":{}},{"cell_type":"markdown","source":"# --- Section 1: Setting Up Google Generative AI ---\n\n## Setting Up Google Generative AI\n\nThe required **Google Generative AI** library was installed using the command `!pip install -U -q \"google-genai==1.7.0\"`. This package provides access to Google’s generative AI models for various NLP tasks.\n\nAfter the installation, the necessary libraries, including **google.generativeai**, were imported for interacting with the AI models. The API key was securely retrieved from **Kaggle Secrets** to authenticate access to the Google AI services.\n\nFinally, the version of the **Gemini SDK** was verified to ensure that the correct library version was being used, confirming that the setup was successfully completed and ready for model interactions.\n","metadata":{}},{"cell_type":"code","source":"!pip install -U -q \"google-genai==1.7.0\"","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-18T09:57:44.907973Z","iopub.execute_input":"2025-04-18T09:57:44.908658Z","iopub.status.idle":"2025-04-18T09:57:48.757833Z","shell.execute_reply.started":"2025-04-18T09:57:44.90863Z","shell.execute_reply":"2025-04-18T09:57:48.756681Z"}},"outputs":[],"execution_count":129},{"cell_type":"code","source":"# Import required libraries\nimport google.generativeai as ggenai\nfrom google.genai import types\nfrom google import genai\nfrom IPython.display import Markdown, display\nfrom kaggle_secrets import UserSecretsClient\nimport numpy as np\nimport json\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T09:57:48.759666Z","iopub.execute_input":"2025-04-18T09:57:48.759999Z","iopub.status.idle":"2025-04-18T09:57:48.765924Z","shell.execute_reply.started":"2025-04-18T09:57:48.759971Z","shell.execute_reply":"2025-04-18T09:57:48.76487Z"}},"outputs":[],"execution_count":130},{"cell_type":"code","source":"# Load API key securely from Kaggle Secrets\nclient = genai.Client(api_key=UserSecretsClient().get_secret(\"GOOGLE_API_KEY\"))\n\n# Configure Google Generative AI\nggenai.configure(api_key=GOOGLE_API_KEY)\n\n# Display Gemini SDK version\nprint(\"Gemini Library Version:\", ggenai.__version__)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T10:06:28.594976Z","iopub.execute_input":"2025-04-18T10:06:28.595321Z","iopub.status.idle":"2025-04-18T10:06:28.891037Z","shell.execute_reply.started":"2025-04-18T10:06:28.595297Z","shell.execute_reply":"2025-04-18T10:06:28.8901Z"}},"outputs":[{"name":"stdout","text":"Gemini Library Version: 0.8.4\n","output_type":"stream"}],"execution_count":148},{"cell_type":"markdown","source":"# --- Section 2: Data Preparation & Embedding Setup ---\n## Data Preparation & Embedding Setup\n\nThe text of **\"Alice’s Adventures in Wonderland\"** was obtained from Project Gutenberg. If the file was not already present locally, it was downloaded and stored.\n\nOnce the transcript was available, it was read into memory to serve as the knowledge base for the chatbot.\n\nTo enable semantic search and retrieval, text embeddings were generated using **Google's Generative AI embedding model**. The Gemini model was used to process the content and convert it into high-dimensional vectors optimized for document retrieval tasks.\n","metadata":{}},{"cell_type":"code","source":"import os\nimport requests\n\nfile_name = \"alice_in_wonderland.txt\"\nurl = \"https://www.gutenberg.org/files/11/11-0.txt\"\n\n# Check if file exists; if not, download it\nif not os.path.exists(file_name):\n    print(\"File not found. Downloading...\")\n    response = requests.get(url)\n    if response.status_code == 200:\n        with open(file_name, 'w', encoding='utf-8') as f:\n            f.write(response.text)\n        print(\"Download successful!\")\n    else:\n        print(\"Failed to download the file. Status code:\", response.status_code)\n\n# Try to read the file\nif os.path.exists(file_name):\n    print(\"Reading Alice in Wonderland transcript...\")\n    with open(file_name, 'r', encoding='utf-8') as f:\n        transcript = f.read()\nelse:\n    print(\"Error: Transcript not available.\")\n    transcript = \"\"  # Safe fallback\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T09:57:49.067383Z","iopub.execute_input":"2025-04-18T09:57:49.067619Z","iopub.status.idle":"2025-04-18T09:57:49.076242Z","shell.execute_reply.started":"2025-04-18T09:57:49.067601Z","shell.execute_reply":"2025-04-18T09:57:49.075315Z"}},"outputs":[{"name":"stdout","text":"Reading Alice in Wonderland transcript...\n","output_type":"stream"}],"execution_count":132},{"cell_type":"code","source":"# Load the Gemini language model\nmodel = ggenai.GenerativeModel(\"gemini-2.0-flash\")\n\n# Generate an embedding\nembedding_response = ggenai.embed_content(\n    model=\"models/embedding-001\",\n    content=\"Sample text to embed\",\n    task_type=\"retrieval_document\"\n)\n\nembedding_vector = embedding_response['embedding']\nprint(\"Vector size:\", len(embedding_vector))\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T09:57:49.077026Z","iopub.execute_input":"2025-04-18T09:57:49.077316Z","iopub.status.idle":"2025-04-18T09:57:49.332141Z","shell.execute_reply.started":"2025-04-18T09:57:49.077297Z","shell.execute_reply":"2025-04-18T09:57:49.331337Z"}},"outputs":[{"name":"stdout","text":"Vector size: 768\n","output_type":"stream"}],"execution_count":133},{"cell_type":"markdown","source":"# --- Section 3: Vector Store Setup ---\n## Vector Store Setup\n\n### Create Embeddings\n\nVector embeddings are created for each transcript chunk using a Google text embedding model. These embeddings represent the text in a high-dimensional space, enabling efficient similarity search and retrieval.\n\n### Create Embeddings with Google GenAI\n\nGoogle GenAI is used to generate embeddings for the text chunks. The resulting vectors are stored in a vector store, allowing for quick and relevant context retrieval during query processing.\n","metadata":{}},{"cell_type":"code","source":"for model in client.models.list():\n  if 'embedContent' in model.supported_actions:\n    print(model.name)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T09:56:48.776506Z","iopub.execute_input":"2025-04-18T09:56:48.776753Z","iopub.status.idle":"2025-04-18T09:56:48.854369Z","shell.execute_reply.started":"2025-04-18T09:56:48.776736Z","shell.execute_reply":"2025-04-18T09:56:48.853445Z"}},"outputs":[{"name":"stdout","text":"models/embedding-001\nmodels/text-embedding-004\nmodels/gemini-embedding-exp-03-07\nmodels/gemini-embedding-exp\n","output_type":"stream"}],"execution_count":122},{"cell_type":"code","source":"embedding_model_name = \"text-embedding-004\"\nembeddings = []\nchunk_size = 500\nchunk_overlap = 50\nchunks = []\nif transcript:  # Only proceed if the transcript was loaded\n    for i in range(0, len(transcript), chunk_size - chunk_overlap):\n        chunk = transcript[i:i + chunk_size]\n        chunks.append(chunk)\n\n    print(f\"Number of transcript chunks: {len(chunks)}\")\n    print(f\"First chunk: {chunks[0][:100]}...\")\nelse:\n    print(\"Transcript is empty. Skipping chunking.\")\n    chunks = []","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T09:57:49.333193Z","iopub.execute_input":"2025-04-18T09:57:49.333489Z","iopub.status.idle":"2025-04-18T09:57:49.340446Z","shell.execute_reply.started":"2025-04-18T09:57:49.333463Z","shell.execute_reply":"2025-04-18T09:57:49.339376Z"}},"outputs":[{"name":"stdout","text":"Number of transcript chunks: 322\nFirst chunk: *** START OF THE PROJECT GUTENBERG EBOOK 11 ***\n\n[Illustration]\n\n\n\n\nAlice’s Adventures in Wonderland...\n","output_type":"stream"}],"execution_count":134},{"cell_type":"code","source":"print(chunks[:5])  # Print the first 5 chunks to inspect their content\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T09:57:49.341671Z","iopub.execute_input":"2025-04-18T09:57:49.341944Z","iopub.status.idle":"2025-04-18T09:57:49.357737Z","shell.execute_reply.started":"2025-04-18T09:57:49.341912Z","shell.execute_reply":"2025-04-18T09:57:49.356788Z"}},"outputs":[{"name":"stdout","text":"['*** START OF THE PROJECT GUTENBERG EBOOK 11 ***\\n\\n[Illustration]\\n\\n\\n\\n\\nAlice’s Adventures in Wonderland\\n\\nby Lewis Carroll\\n\\nTHE MILLENNIUM FULCRUM EDITION 3.0\\n\\nContents\\n\\n CHAPTER I.     Down the Rabbit-Hole\\n CHAPTER II.    The Pool of Tears\\n CHAPTER III.   A Caucus-Race and a Long Tale\\n CHAPTER IV.    The Rabbit Sends in a Little Bill\\n CHAPTER V.     Advice from a Caterpillar\\n CHAPTER VI.    Pig and Pepper\\n CHAPTER VII.   A Mad Tea-Party\\n CHAPTER VIII.  The Queen’s Croquet-Ground\\n CHAPTER IX.    The', 'I.  The Queen’s Croquet-Ground\\n CHAPTER IX.    The Mock Turtle’s Story\\n CHAPTER X.     The Lobster Quadrille\\n CHAPTER XI.    Who Stole the Tarts?\\n CHAPTER XII.   Alice’s Evidence\\n\\n\\n\\n\\nCHAPTER I.\\nDown the Rabbit-Hole\\n\\n\\nAlice was beginning to get very tired of sitting by her sister on the\\nbank, and of having nothing to do: once or twice she had peeped into\\nthe book her sister was reading, but it had no pictures or\\nconversations in it, “and what is the use of a book,” thought Alice\\n“without pictures', 'he use of a book,” thought Alice\\n“without pictures or conversations?”\\n\\nSo she was considering in her own mind (as well as she could, for the\\nhot day made her feel very sleepy and stupid), whether the pleasure of\\nmaking a daisy-chain would be worth the trouble of getting up and\\npicking the daisies, when suddenly a White Rabbit with pink eyes ran\\nclose by her.\\n\\nThere was nothing so _very_ remarkable in that; nor did Alice think it\\nso _very_ much out of the way to hear the Rabbit say to itself, “Oh', 't of the way to hear the Rabbit say to itself, “Oh\\ndear! Oh dear! I shall be late!” (when she thought it over afterwards,\\nit occurred to her that she ought to have wondered at this, but at the\\ntime it all seemed quite natural); but when the Rabbit actually _took a\\nwatch out of its waistcoat-pocket_, and looked at it, and then hurried\\non, Alice started to her feet, for it flashed across her mind that she\\nhad never before seen a rabbit with either a waistcoat-pocket, or a\\nwatch to take out of it, ', 'a waistcoat-pocket, or a\\nwatch to take out of it, and burning with curiosity, she ran across the\\nfield after it, and fortunately was just in time to see it pop down a\\nlarge rabbit-hole under the hedge.\\n\\nIn another moment down went Alice after it, never once considering how\\nin the world she was to get out again.\\n\\nThe rabbit-hole went straight on like a tunnel for some way, and then\\ndipped suddenly down, so suddenly that Alice had not a moment to think\\nabout stopping herself before she found herse']\n","output_type":"stream"}],"execution_count":135},{"cell_type":"code","source":"    embeddings = []\n    batch_size = 100\n    num_chunks = len(chunks)\n    for i in range(0, num_chunks, batch_size):\n        batch = chunks[i:i + batch_size]\n        try:\n            response = client.models.embed_content(model=embedding_model_name, contents=batch)\n            embeddings.extend([embedding.values for embedding in response.embeddings])\n        except Exception as e:\n            print(f\"Error creating embeddings for batch {i // batch_size}: {e}\")\n            Markdown(f\"Error creating embeddings for batch {i // batch_size}: {e}\")\n            # Optionally, you could implement more sophisticated error handling here\n\n    embeddings = np.array(embeddings).astype(\"float32\") if embeddings else np.array([])\n    embedding_dimension = embeddings.shape[1] if embeddings.size > 0 else 0\n    print(f\"Generated embeddings for {len(embeddings)} chunks with dimension {embedding_dimension}.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T09:57:49.358687Z","iopub.execute_input":"2025-04-18T09:57:49.359023Z","iopub.status.idle":"2025-04-18T09:57:52.662083Z","shell.execute_reply.started":"2025-04-18T09:57:49.359001Z","shell.execute_reply":"2025-04-18T09:57:52.661265Z"}},"outputs":[{"name":"stdout","text":"Generated embeddings for 322 chunks with dimension 768.\n","output_type":"stream"}],"execution_count":136},{"cell_type":"code","source":"def chunk_text(text, chunk_size=1500):\n    \"\"\"Chunk text into smaller pieces\"\"\"\n    # Split text into paragraphs\n    paragraphs = text.split(\"\\n\")\n    chunks = []\n    current_chunk = \"\"\n    \n    for paragraph in paragraphs:\n        if len(current_chunk) + len(paragraph) > chunk_size:\n            chunks.append(current_chunk)\n            current_chunk = paragraph\n        else:\n            current_chunk += \"\\n\" + paragraph\n    \n    if current_chunk:\n        chunks.append(current_chunk)  # Add any remaining content as a chunk\n    \n    return chunks\n\n# Create chunks from the Alice in Wonderland transcript\nchunks = chunk_text(transcript)\n\nprint(f\"Total chunks created: {len(chunks)}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T09:57:52.66309Z","iopub.execute_input":"2025-04-18T09:57:52.663451Z","iopub.status.idle":"2025-04-18T09:57:52.672149Z","shell.execute_reply.started":"2025-04-18T09:57:52.663428Z","shell.execute_reply":"2025-04-18T09:57:52.671142Z"}},"outputs":[{"name":"stdout","text":"Total chunks created: 99\n","output_type":"stream"}],"execution_count":137},{"cell_type":"markdown","source":"# --- Section 4: RAG Pipeline with Function Calling ---\n## RAG Pipeline with Function Calling\n\nThe **Retrieval Augmented Generation (RAG)** pipeline is implemented to combine information retrieval with generative models, enhancing the chatbot’s performance. The pipeline retrieves relevant context from a knowledge base and then generates accurate, context-aware responses using a language model.\n\n**Function calling** is integrated into the pipeline to structure and control the output effectively. This allows interaction with specific tasks or services, ensuring that the generated answers align closely with the query and the retrieved context.\n\nThis section includes the function definition for the RAG pipeline and shows how it enables dynamic response generation based on input queries.\n","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics.pairwise import cosine_similarity\n\n# Embed any text using Google GenAI\ndef embed_text(text, model_name=\"text-embedding-004\"):\n    embedding_response = ggenai.embed_content(\n        model=f\"models/{model_name}\",\n        content=text,\n        task_type=\"retrieval_document\"\n    )\n    return np.array(embedding_response['embedding']).astype('float32')\n\n# Find top_k relevant chunks using cosine similarity\ndef get_relevant_chunks_with_embeddings(query, chunks, top_k=3):\n    query_embedding = embed_text(query)\n    chunk_embeddings = [embed_text(chunk) for chunk in chunks]\n\n    similarities = cosine_similarity([query_embedding], chunk_embeddings)[0]\n    top_indices = similarities.argsort()[-top_k:][::-1]\n    \n    return [chunks[i] for i in top_indices]\n\n# Generate the final response using Gemini\ndef generate_answer_with_gemini(query, relevant_chunks, model):\n    context = \"\\n\".join(relevant_chunks)\n    prompt = f\"\"\"Answer the following question based on the context below:\n\nContext:\n{context}\n\nQuestion: {query}\n\nAnswer in a clear, concise manner.\"\"\"\n    \n    response = model.generate_content(prompt)\n    return response.text\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T09:57:52.674402Z","iopub.execute_input":"2025-04-18T09:57:52.675163Z","iopub.status.idle":"2025-04-18T09:57:52.691861Z","shell.execute_reply.started":"2025-04-18T09:57:52.675136Z","shell.execute_reply":"2025-04-18T09:57:52.690973Z"}},"outputs":[],"execution_count":138},{"cell_type":"markdown","source":"# --- Section 5: Demo Queries ---\n## Demo Queries\n\nThis section tests the chatbot with a couple of demo queries to evaluate its performance in generating responses using the **RAG (Retrieval-Augmented Generation)** pipeline. The pipeline retrieves relevant context from the knowledge base and generates responses accordingly.\n\n### Query 1: \"What made Alice's neck grow long?\"\n\nThe first query investigates the cause behind Alice's neck growing long. The chatbot retrieves context from the knowledge base and generates an answer based on the relevant information.\n\n### Query 2: \"Describe the Queen of Hearts' personality.\"\n\nThe second query examines the personality traits of the Queen of Hearts. The chatbot analyzes the relevant context and generates a response that highlights her key characteristics, including her mood and behavior.\n\nThese demo queries help assess how the chatbot effectively retrieves context and generates answers in real-time.\n","metadata":{}},{"cell_type":"code","source":"from IPython.display import Markdown, display\n\n# Query 1\nquery1 = \"What made Alice's neck grow long?\"\nrelevant_chunks1 = get_relevant_chunks_with_embeddings(query1, chunks, top_k=3)\nanswer1 = generate_answer_with_gemini(query1, relevant_chunks1, model)\n\ndisplay(Markdown(f\"**Query:** {query1}\"))\ndisplay(Markdown(f\"**Relevant Context:**\\n\\n{''.join(relevant_chunks1)}\"))\ndisplay(Markdown(f\"**Answer:** {answer1}\"))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T09:57:52.692691Z","iopub.execute_input":"2025-04-18T09:57:52.692909Z","iopub.status.idle":"2025-04-18T09:58:15.622476Z","shell.execute_reply.started":"2025-04-18T09:57:52.692893Z","shell.execute_reply":"2025-04-18T09:58:15.621599Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"**Query:** What made Alice's neck grow long?"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"**Relevant Context:**\n\nShe was a good deal frightened by this very sudden change, but she felt\nthat there was no time to be lost, as she was shrinking rapidly; so she\nset to work at once to eat some of the other bit. Her chin was pressed\nso closely against her foot, that there was hardly room to open her\nmouth; but she did it at last, and managed to swallow a morsel of the\nlefthand bit.\n\n*      *      *      *      *      *      *\n\n    *      *      *      *      *      *\n\n*      *      *      *      *      *      *\n\n\n“Come, my head’s free at last!” said Alice in a tone of delight, which\nchanged into alarm in another moment, when she found that her shoulders\nwere nowhere to be found: all she could see, when she looked down, was\nan immense length of neck, which seemed to rise like a stalk out of a\nsea of green leaves that lay far below her.\n\n“What _can_ all that green stuff be?” said Alice. “And where _have_ my\nshoulders got to? And oh, my poor hands, how is it I can’t see you?”\nShe was moving them about as she spoke, but no result seemed to follow,\nexcept a little shaking among the distant green leaves.\n\nAs there seemed to be no chance of getting her hands up to her head,\nshe tried to get her head down to them, and was delighted to find that\nher neck would bend about easily in any direction, like a serpent. She\nhad just succeeded in curving it down into a graceful zigzag, and was\ngoing to dive in among the leaves, which she found to be nothing but“How doth the little crocodile\n    Improve his shining tail,\nAnd pour the waters of the Nile\n    On every golden scale!\n\n“How cheerfully he seems to grin,\n    How neatly spread his claws,\nAnd welcome little fishes in\n    With gently smiling jaws!”\n\n\n“I’m sure those are not the right words,” said poor Alice, and her eyes\nfilled with tears again as she went on, “I must be Mabel after all, and\nI shall have to go and live in that poky little house, and have next to\nno toys to play with, and oh! ever so many lessons to learn! No, I’ve\nmade up my mind about it; if I’m Mabel, I’ll stay down here! It’ll be\nno use their putting their heads down and saying ‘Come up again, dear!’\nI shall only look up and say ‘Who am I then? Tell me that first, and\nthen, if I like being that person, I’ll come up: if not, I’ll stay down\nhere till I’m somebody else’—but, oh dear!” cried Alice, with a sudden\nburst of tears, “I do wish they _would_ put their heads down! I am so\n_very_ tired of being all alone here!”\n\nAs she said this she looked down at her hands, and was surprised to see\nthat she had put on one of the Rabbit’s little white kid gloves while\nshe was talking. “How _can_ I have done that?” she thought. “I must be\ngrowing small again.” She got up and went to the table to measure\nherself by it, and found that, as nearly as she could guess, she was\nnow about two feet high, and was going on shrinking rapidly: she soon\nfound out that the cause of this was the fan she was holding, and shedrunk half the bottle, she found her head pressing against the ceiling,\nand had to stoop to save her neck from being broken. She hastily put\ndown the bottle, saying to herself “That’s quite enough—I hope I shan’t\ngrow any more—As it is, I can’t get out at the door—I do wish I hadn’t\ndrunk quite so much!”\n\nAlas! it was too late to wish that! She went on growing, and growing,\nand very soon had to kneel down on the floor: in another minute there\nwas not even room for this, and she tried the effect of lying down with\none elbow against the door, and the other arm curled round her head.\nStill she went on growing, and, as a last resource, she put one arm out\nof the window, and one foot up the chimney, and said to herself “Now I\ncan do no more, whatever happens. What _will_ become of me?”\n\nLuckily for Alice, the little magic bottle had now had its full effect,\nand she grew no larger: still it was very uncomfortable, and, as there\nseemed to be no sort of chance of her ever getting out of the room\nagain, no wonder she felt unhappy.\n\n“It was much pleasanter at home,” thought poor Alice, “when one wasn’t\nalways growing larger and smaller, and being ordered about by mice and\nrabbits. I almost wish I hadn’t gone down that rabbit-hole—and yet—and\nyet—it’s rather curious, you know, this sort of life! I do wonder what\n_can_ have happened to me! When I used to read fairy-tales, I fancied\nthat kind of thing never happened, and now here I am in the middle of"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"**Answer:** Eating the morsel of the lefthand bit made Alice's neck grow long.\n"},"metadata":{}}],"execution_count":139},{"cell_type":"code","source":"# Query 2\nquery2 = \"Describe the Queen of Hearts' personality.\"\nrelevant_chunks2 = get_relevant_chunks_with_embeddings(query2, chunks, top_k=3)\nanswer2 = generate_answer_with_gemini(query2, relevant_chunks2, model)\n\ndisplay(Markdown(f\"**Query:** {query2}\"))\ndisplay(Markdown(f\"**Relevant Context:**\\n\\n{''.join(relevant_chunks2)}\"))\ndisplay(Markdown(f\"**Answer:** {answer2}\"))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T09:58:15.623504Z","iopub.execute_input":"2025-04-18T09:58:15.62443Z","iopub.status.idle":"2025-04-18T09:58:37.638596Z","shell.execute_reply.started":"2025-04-18T09:58:15.624408Z","shell.execute_reply":"2025-04-18T09:58:37.637853Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"**Query:** Describe the Queen of Hearts' personality."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"**Relevant Context:**\n\nFirst came ten soldiers carrying clubs; these were all shaped like the\nthree gardeners, oblong and flat, with their hands and feet at the\ncorners: next the ten courtiers; these were ornamented all over with\ndiamonds, and walked two and two, as the soldiers did. After these came\nthe royal children; there were ten of them, and the little dears came\njumping merrily along hand in hand, in couples: they were all\nornamented with hearts. Next came the guests, mostly Kings and Queens,\nand among them Alice recognised the White Rabbit: it was talking in a\nhurried nervous manner, smiling at everything that was said, and went\nby without noticing her. Then followed the Knave of Hearts, carrying\nthe King’s crown on a crimson velvet cushion; and, last of all this\ngrand procession, came THE KING AND QUEEN OF HEARTS.\n\nAlice was rather doubtful whether she ought not to lie down on her face\nlike the three gardeners, but she could not remember ever having heard\nof such a rule at processions; “and besides, what would be the use of a\nprocession,” thought she, “if people had all to lie down upon their\nfaces, so that they couldn’t see it?” So she stood still where she was,\nand waited.\n\nWhen the procession came opposite to Alice, they all stopped and looked\nat her, and the Queen said severely “Who is this?” She said it to the\nKnave of Hearts, who only bowed and smiled in reply.\n\n“Idiot!” said the Queen, tossing her head impatiently; and, turning to\nAlice, she went on, “What’s your name, child?”\n“My name is Alice, so please your Majesty,” said Alice very politely;\nbut she added, to herself, “Why, they’re only a pack of cards, after\nall. I needn’t be afraid of them!”\n\n“And who are _these?_” said the Queen, pointing to the three gardeners\nwho were lying round the rose-tree; for, you see, as they were lying on\ntheir faces, and the pattern on their backs was the same as the rest of\nthe pack, she could not tell whether they were gardeners, or soldiers,\nor courtiers, or three of her own children.\n\n“How should _I_ know?” said Alice, surprised at her own courage. “It’s\nno business of _mine_.”\n\nThe Queen turned crimson with fury, and, after glaring at her for a\nmoment like a wild beast, screamed “Off with her head! Off—”\n\n“Nonsense!” said Alice, very loudly and decidedly, and the Queen was\nsilent.\n\nThe King laid his hand upon her arm, and timidly said “Consider, my\ndear: she is only a child!”\n\nThe Queen turned angrily away from him, and said to the Knave “Turn\nthem over!”\n\nThe Knave did so, very carefully, with one foot.\n\n“Get up!” said the Queen, in a shrill, loud voice, and the three\ngardeners instantly jumped up, and began bowing to the King, the Queen,\nthe royal children, and everybody else.\n\n“Leave off that!” screamed the Queen. “You make me giddy.” And then,\nturning to the rose-tree, she went on, “What _have_ you been doing\nhere?”\n\n“May it please your Majesty,” said Two, in a very humble tone, going\ndown on one knee as he spoke, “we were trying—”\nalways getting up and walking off to other parts of the ground, Alice\nsoon came to the conclusion that it was a very difficult game indeed.\n\nThe players all played at once without waiting for turns, quarrelling\nall the while, and fighting for the hedgehogs; and in a very short time\nthe Queen was in a furious passion, and went stamping about, and\nshouting “Off with his head!” or “Off with her head!” about once in a\nminute.\n\nAlice began to feel very uneasy: to be sure, she had not as yet had any\ndispute with the Queen, but she knew that it might happen any minute,\n“and then,” thought she, “what would become of me? They’re dreadfully\nfond of beheading people here; the great wonder is, that there’s any\none left alive!”\n\nShe was looking about for some way of escape, and wondering whether she\ncould get away without being seen, when she noticed a curious\nappearance in the air: it puzzled her very much at first, but, after\nwatching it a minute or two, she made it out to be a grin, and she said\nto herself “It’s the Cheshire Cat: now I shall have somebody to talk\nto.”\n\n“How are you getting on?” said the Cat, as soon as there was mouth\nenough for it to speak with.\n\nAlice waited till the eyes appeared, and then nodded. “It’s no use\nspeaking to it,” she thought, “till its ears have come, or at least one\nof them.” In another minute the whole head appeared, and then Alice put\ndown her flamingo, and began an account of the game, feeling very glad"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"**Answer:** The Queen of Hearts is easily angered, impatient, and prone to shouting. She is quick to order executions, making her seem tyrannical and unreasonable.\n"},"metadata":{}}],"execution_count":140},{"cell_type":"markdown","source":"# --- Section 6: Gen AI Evaluation ---\n## Gen AI Evaluation\n\nThis section evaluates the performance of the chatbot using **Gen AI**. The goal is to assess how effectively the generated responses meet predefined quality criteria such as relevance, accuracy, coherence, fluency, and conciseness.\n\n### Pointwise Evaluation\n\nA **pointwise evaluation** is performed to analyze individual question-answer pairs based on the following criteria:\n\n- **Relevance**: Does the response appropriately address the query?\n- **Accuracy**: Is the information provided correct?\n- **Coherence**: Is the answer logically structured and easy to understand?\n- **Fluency**: Is the language natural and error-free?\n- **Conciseness**: Is the response clear and to the point?\n- **Quality**: Does the answer provide valuable insights based on the query?\n\n### Example Queries and Evaluations\n\nThe chatbot’s responses to the following two sample queries are evaluated:\n\n1. **Query 1**: \"What made Alice's neck grow long?\"\n2. **Query 2**: \"Describe the Queen of Hearts' personality.\"\n\n### Summary of Gen AI Evaluations\n\nThis evaluation provides insights into the chatbot’s performance in generating responses that are relevant, coherent, and accurate. The results highlight areas for improvement, such as refining responses to be more precise and adding depth where needed.\n\nThe overall goal is to continuously refine the model’s ability to generate meaningful, contextually relevant answers that address user queries effectively.\n","metadata":{}},{"cell_type":"code","source":"def evaluate_response(query, relevant_context, answer):\n    \"\"\"\n    Enhanced pointwise evaluation function that returns qualitative labels and numerical scores.\n    \"\"\"\n    evaluation = {}\n    score = 0\n\n    if not answer:\n        return {\n            \"relevance\": (\"Answer is missing\", 0),\n            \"coherence\": (\"Answer is missing\", 0),\n            \"accuracy\": (\"Answer is missing\", 0),\n            \"overall_score_out_of_3\": 0\n        }\n\n    # --- Relevance Check ---\n    if any(word.lower() in answer.lower() for word in query.lower().split()):\n        evaluation[\"relevance\"] = (\"Likely Relevant\", 1)\n        score += 1\n    else:\n        evaluation[\"relevance\"] = (\"Potentially Irrelevant\", 0)\n\n    # --- Coherence Check ---\n    if len(answer.split()) > 5:\n        evaluation[\"coherence\"] = (\"Likely Coherent\", 1)\n        score += 1\n    else:\n        evaluation[\"coherence\"] = (\"Potentially Incoherent\", 0)\n\n    # --- Accuracy Placeholder ---\n    evaluation[\"accuracy\"] = (\"Requires Manual Inspection\", 0.5)\n    score += 0.5\n\n    # --- Final Score ---\n    evaluation[\"overall_score_out_of_3\"] = round(score, 2)\n\n    return evaluation\n\n\n# Evaluate responses\nevaluation1 = evaluate_response(query1, relevant_chunks1, answer1)\nevaluation2 = evaluate_response(query2, relevant_chunks2, answer2)\n\n# Display evaluations using Markdown\ndisplay(Markdown(f\"### Evaluation - Query 1 (`{query1}`)\"))\nfor k, v in evaluation1.items():\n    display(Markdown(f\"**{k.replace('_', ' ').capitalize()}**: {v[0] if isinstance(v, tuple) else v}\"))\n\ndisplay(Markdown(f\"### Evaluation - Query 2 (`{query2}`)\"))\nfor k, v in evaluation2.items():\n    display(Markdown(f\"**{k.replace('_', ' ').capitalize()}**: {v[0] if isinstance(v, tuple) else v}\"))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T09:58:37.639695Z","iopub.execute_input":"2025-04-18T09:58:37.639928Z","iopub.status.idle":"2025-04-18T09:58:37.663351Z","shell.execute_reply.started":"2025-04-18T09:58:37.639911Z","shell.execute_reply":"2025-04-18T09:58:37.662269Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"### Evaluation - Query 1 (`What made Alice's neck grow long?`)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"**Relevance**: Likely Relevant"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"**Coherence**: Likely Coherent"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"**Accuracy**: Requires Manual Inspection"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"**Overall score out of 3**: 2.5"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"### Evaluation - Query 2 (`Describe the Queen of Hearts' personality.`)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"**Relevance**: Likely Relevant"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"**Coherence**: Likely Coherent"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"**Accuracy**: Requires Manual Inspection"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"**Overall score out of 3**: 2.5"},"metadata":{}}],"execution_count":141},{"cell_type":"code","source":"def evaluate_with_genai(model, query, context, answer):\n    if isinstance(context, list):\n        context = \"\\n\".join(context)  # Join list into one string with line breaks\n\n    # Check if the answer is None or empty and set a fallback message\n    if not answer:\n        answer = \"No answer provided.\"\n\n    # Define a relevant evaluation prompt specific to Gen AI\n    eval_prompt = \"\"\"\n    Evaluate the following response based on the given query and context:\n    Query: {query}\n    Relevant Context: {relevant_context}\n    Answer: {answer}\n\n    Please assess the following criteria:\n    - Instruction Following\n    - Groundedness (Does the answer contain relevant context?)\n    - Fluency (How well-formed and readable is the answer?)\n    - Conciseness (Is the answer verbose?)\n    - Quality (Is the response informative and accurate?)\n\n    Provide a detailed evaluation in a clear format:\n    \"\"\".format(\n        query=query.strip(),\n        relevant_context=context.strip(),\n        answer=answer.strip()\n    )\n    \n    # Use Gen AI model to generate evaluation\n    response = model.generate_content(eval_prompt)\n    return response.text\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T09:58:37.664341Z","iopub.execute_input":"2025-04-18T09:58:37.664737Z","iopub.status.idle":"2025-04-18T09:58:37.672712Z","shell.execute_reply.started":"2025-04-18T09:58:37.664697Z","shell.execute_reply":"2025-04-18T09:58:37.671805Z"}},"outputs":[],"execution_count":142},{"cell_type":"code","source":"# Evaluate responses using the Gen AI model\ngenai_eval_1 = evaluate_with_genai(model, query1, relevant_context1, answer1)\ngenai_eval_2 = evaluate_with_genai(model, query2, relevant_context2, answer2)\n\n# Display results in markdown format\ndisplay(Markdown(\"### Gen AI Evaluation - Query 1\"))\ndisplay(Markdown(genai_eval_1))\n\ndisplay(Markdown(\"### Gen AI Evaluation - Query 2\"))\ndisplay(Markdown(genai_eval_2))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T09:58:37.673571Z","iopub.execute_input":"2025-04-18T09:58:37.673854Z","iopub.status.idle":"2025-04-18T09:58:42.387068Z","shell.execute_reply.started":"2025-04-18T09:58:37.673825Z","shell.execute_reply":"2025-04-18T09:58:42.386396Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"### Gen AI Evaluation - Query 1"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"## Evaluation of the Answer:\n\n**Query:** What made Alice's neck grow long?\n**Relevant Context:** Context related to Alice's transformation in 'Alice in Wonderland'.\n**Answer:** Eating the morsel of the lefthand bit made Alice's neck grow long.\n\nHere's an evaluation based on the provided criteria:\n\n*   **Instruction Following:** The response directly answers the query. It identifies the cause of Alice's neck elongation.\n\n*   **Groundedness:** The answer is grounded in the relevant context of Alice in Wonderland. The transformations are a core element of the story. However, the detail of \"lefthand bit\" might be slightly inaccurate or a less common recollection. More broadly, it would be more accurate to say something that was labeled \"Eat Me.\"\n\n*   **Fluency:** The answer is well-formed and easy to read. The language is clear and understandable.\n\n*   **Conciseness:** The answer is concise and avoids unnecessary details. It gets straight to the point.\n\n*   **Quality:** The response is generally informative and mostly accurate. It correctly identifies the food as the cause of Alice's neck growth. However, the phrasing \"lefthand bit\" could be potentially misleading or not the most accurate description. Ideally, the response should mention \"Eat Me.\"\n\n**Overall Assessment:**\n\nThe response is good. It successfully answers the query within the given context and does so in a fluent and concise manner. The only minor drawback is the slightly imprecise detail about the \"lefthand bit\" that could benefit from being more accurate or referring to the \"Eat Me\" label.\n\n**Suggestions for Improvement:**\n\n*   Replace \"Eating the morsel of the lefthand bit\" with \"Eating a cake labeled 'Eat Me'\" or \"Eating a piece of cake labeled 'Eat Me'\". This would increase the accuracy and clarity of the response.\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"### Gen AI Evaluation - Query 2"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"## Evaluation of the Response\n\n**Query:** Describe the Queen of Hearts' personality.\n**Relevant Context:** Context related to the Queen of Hearts in 'Alice in Wonderland'.\n**Answer:** The Queen of Hearts is easily angered, impatient, and prone to shouting. She is quick to order executions, making her seem tyrannical and unreasonable.\n\nHere's a detailed evaluation based on the specified criteria:\n\n*   **Instruction Following:** The response directly addresses the query by describing the Queen of Hearts' personality. It follows the instructions effectively.\n\n*   **Groundedness:** The response is firmly grounded in the context of the Queen of Hearts from *Alice in Wonderland*. All the described traits (easily angered, impatient, prone to shouting, ordering executions, tyrannical, unreasonable) are consistent with her portrayal in the story.\n\n*   **Fluency:** The response is fluent and well-written. The sentences are grammatically correct and easy to understand. The language used is appropriate for describing a character's personality.\n\n*   **Conciseness:** The response is concise and to the point. It efficiently conveys the key aspects of the Queen's personality without being overly verbose.\n\n*   **Quality:** The response is informative and accurate. It provides a good summary of the Queen of Hearts' personality, highlighting her dominant and defining characteristics. The description captures the essence of the character as portrayed in the book.\n\n**Overall:**\n\nThe response is excellent. It accurately and concisely answers the query while remaining grounded in the relevant context. The writing is fluent and the information provided is of high quality.\n"},"metadata":{}}],"execution_count":143}]}